[{"categories":null,"content":"标题H1 标题H2 标题H3222 \n1djifjafjdfisdhfdi\ndjifdhifhsifhs\n2dfshsifhsfso\nidhfsihfsihs\n3djifhsofjsof\ncjifjwofewfuworw\ndjfofuoiwufwo\njeifwofuwfuwoufwow\nwofuwofuwofwofuwo\njiwchiowhwihwihih\n一般，当我们提交一个应用(jar包，py脚本)时，是首先提交到Master(Cluster Manager) ,然后Master会随机选择一个worker去启动Driver(因为Driver是负责收集最后结果数据的，所以要注意OOM问题)，Driver负责创建SparkContext和SparkSession之类的，同时根据任务创建RDD和一系列操作，并把这些信息发送到所谓的Master结点,也即是上图的Cluster Manager(如yarn,Mesos等), Cluster Manager负责整个集群的资源管理，根据应用提交的配置，分配一系列的worker结点(这个一般指物理机，其中会有一个后台进程NodeManager与Cluster Manager进行通信)，同时在worker里面根据内存要求大小等配置，创建executor(对应一个JVM)，一个worker里面可以有分属于不同应用的executor(不同任务数据彼此不共享)。随后，Driver通过把应用分成一系列的task，并把信息发到Master，交由Master进行调度，同时把应用的代码序列化到executor中，在executor中会启动若干个task对数据进行处理。 通常User提交一个应用时，Spark会根据action()划分为一个个job，而每一个job根据前后是否需要shuffle划分为一个个stage，每个stage的task数量一般对应于stage最后的RDD算子所要求的partition数量，一个task对应一个partition(因为stage最后的RDD算子通常是产生结果的，task可以通过链式关系追溯每个步骤需要涉及的partition数目)。spark.executor.cores指定executor有多少个core(启用超线程后的逻辑CPU)，spark.task.cpus指定一个task占用多少个core。所以一个executor同时可以有spark.executor.cores/spark.task.cpus个task在执行。 所以在整个spark架构上面有几个重要的角色(不同的模式yarn、standalone等又有所不同)：\n","description":"","tags":null,"title":"spark原理综述","uri":"/posts/spark%E5%8E%9F%E7%90%86%E7%BB%BC%E8%BF%B0/"}]
